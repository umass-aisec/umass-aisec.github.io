{
    "F25": [
        {
            "date": "Mon, Sep 08",
            "date2": "2025/09/08",
            "name": "Ilia Shumailov",
            "affiliation": "Google Deepmind",
            "link": "https://scholar.google.com/citations?user=e-YbZyEAAAAJ",
            "title": "Beyond model.generate(): Can I Even Tell What Is Going On and Why It Matters",
            "abstract": "model.generate() is magic, it works! But why does it work and what is actually hiding behind this simple functional interface? This talk moves beyond model performance to inspect its untrusted origins. We'll explore how today's ML pipelines—built on layers of pre-trained models and third-party dependencies--are a fertile ground for problems.",
            "bio": "Ilia Shumailov is a research scientist formerly at Google DeepMind, where he focused on machine learning security and adversarial vulnerabilities. He completed his PhD in Computer Science at the University of Cambridge in 2021 under the supervision of Professor Ross Anderson. His research explores the intersection of machine learning and computer security. Prior to DeepMind, he held fellowships at Christ Church, University of Oxford, and the Vector Institute."
        },
        {
            "date": "Mon, Sep 15",
            "date2": "2025/09/15",
            "name": "Benjamin Laufer",
            "affiliation": "Cornell Tech",
            "link": "https://bendlaufer.github.io/index.html",
            "title": "AI Ecosystems: Structure, Strategy, Risk and Regulation",
            "abstract": "The development of artificial intelligence is increasingly shaped by interactions between general-purpose model creators, downstream fine-tuners, regulators, and open-source communities. In this talk, I present a line of recent work that develops formal and empirical approaches to understanding these dynamics. First, I introduce a game-theoretic model of adaptation and regulation. Bargaining shapes the division of surplus between upstream creators and downstream adaptors, and regulatory choices around safety investments can impact the equilibrium strategies. A key insight is that weak regulation targeted only at downstream actors can backfire, reducing overall safety—whereas stronger, well-placed standards can align incentives and improve both safety and performance. Second, I turn to an empirical reconstruction of the machine learning ecosystem using 1.86 million models on Hugging Face. By mapping “family trees,” sprawling lineages of fine-tuning, we uncover evolutionary patterns that have implications for safety, security and governance. Ensuring safe and secure AI requires taking an ecosystem-level view, and considering the multiple actors and incentives involved in AI development.",
            "bio": "Benjamin Laufer is a PhD student in the School of Computing and Information Sciences at Cornell Tech, where he is advised by Helen Nissenbaum and Jon Kleinberg, and affiliated with the AI, Policy and Practice Group and the Digital Life Initiative. He is interested in data-driven algorithmic systems and their implications for the public interest. His research uses tools and methods spanning statistics, game theory, network science, and ethics. Prior to joining Cornell, Ben worked as a data scientist at Lime, where he applied machine learning to urban mobility decisions. He graduated from Princeton University with a B.S.E. in Operations Research and Financial Engineering with minors in Urban Studies and Environmental Studies."
        },
        {
            "date": "Mon, Sep 22",
            "date2": "2025/09/22",
            "name": "Ambra Demontis",
            "affiliation": "University of Cagliari",
            "link": "https://sites.unica.it/pralab/people/ambra-demontis/",
            "title": "The Security of Machine Learning against Poisoning",
            "abstract": "Organizations recognize poisoning as one of the attacks against machine learning systems that can affect their business the most. In this talk, I will provide a historical overview of poisoning attacks that manipulate training data to compromise the performance of machine learning systems at test time, along with the defenses developed against them, highlighting the principal research lines. During the talk, I will also provide examples of different typologies of poisoning attacks, including those aimed at slowing down model outputs, poisoning attacks that activate only after model pruning, and those designed against foundation models. Finally, I will discuss the current limitations and open research questions in this field.",
            "bio": "Ambra Demontis is an Assistant Professor at the University of Cagliari, Italy. She received her M.Sc. degree (Hons.) in Computer Science and her Ph.D. degree in Electronic Engineering and Computer Science from the University of Cagliari, Italy, in 2014 and 2018. Her research focuses on the security of machine learning algorithms. These algorithms have reported outstanding performances; however, they can be easily fooled by attackers. The research of Dr. Ambra Demontis has contributed to studying their vulnerabilities to different types of attacks and making them more robust. She serves on the program committee of different conferences and journals. She is Associate Editor of the International Journal of Machine Learning and Cybernetics and the Elsevier Pattern Recognition Journal. She has been co-chair of the AISec workshops (2019-2022), area chair, and track chair of ICPR, and she is the chair of the IAPR TC 1. She is a member of IEEE, ACM, and IAPR."
        },
        {
            "date": "Mon, Sep 29",
            "date2": "2025/09/29",
            "name": "Arman Zharmagambetov",
            "affiliation": "Meta FAIR",
            "link": "https://arman-z.github.io/",
            "title": "Security and Privacy Evaluation of Autonomous AI Agents",
            "abstract": "Autonomous AI agents have the potential to greatly enhance productivity by automating complex, multi-step tasks, but their ability to act on users’ behalf raises significant security and privacy concerns. In this talk, we introduce two new benchmarks—WASP for evaluating agents’ resilience to prompt injection attacks, and AGENTDAM for assessing whether these agents “overshare” sensitive data without permission. Our end-to-end evaluations reveal that even state-of-the-art agents are vulnerable to simple attacks and often misuse sensitive information, highlighting the risks of deploying these systems in real-world scenarios. We also discuss several defense mechanisms and emphasize the need for further research to ensure autonomous AI agents are both secure and privacy-preserving.",
            "bio": "Arman Zharmagambetov is a research scientist in the Fundamental AI Research (FAIR) team at Meta. His research primarily focuses on machine learning and optimization, recently exploring their application in enhancing the security and robustness of AI systems. He received his PhD from the University of California – Merced, , advised by Miguel Carreira-Perpinan. Afterward, he completed his postdoctoral research with Yuandong Tian at FAIR, focusing on AI-guided design and optimization."
        },
        {
            "date": "Mon, Oct 06",
            "date2": "2025/10/06",
            "name": "Madiha Z. Choksi",
            "affiliation": "Cornell Tech",
            "link": "https://madihaz.com/",
            "title": "Terms of Care: Designing Participatory Data Governance for Disability Communities",
            "abstract": "Improving the poor performance of AI media generation for marginalized communities requires data, necessitating communities to balance between increased technological function and risks of data misuse and exploitation. To support marginalized communities to safely engage with AI development, we explore participatory approaches to AI data governance and stewardship with three disability advocacy organizations. We first document governance norms and tensions that shape how “high-context” disability communities can engage with licensing through a set of semi-structured interviews. We then present the results of a series of iterative interventions that led to the creation of adaptable community data licenses that reflect the relational organizational values observed. Synthesizing this work, we contribute (1) a framework for translating values into modular license clauses, and (2) design implications for AI platforms to better support community-led stewardship. We argue that highly contextual license design does not scale; relational, context-sensitive approaches are critical for trustworthy AI governance.",
            "bio": "Madiha Zahrah Choksi is a Ph.D. candidate in Computing and Information Science at Cornell Tech, advised by Helen Nissenbaum and James Grimmelmann. She works on topics at the intersection of technology, privacy, and law, with a focus on community governance. Specifically, she is interested in understanding how online communities use technical and legal affordances to establish norms and express and enact their values with an emphasis on privacy and openness. She has published widely across HCI, law, and AI governance. Her scholarship has earned top paper awards at CHI and ICA and has influenced both platform design and public debate on surveillance. Beyond research, she brings a decade of professional experience spanning government, academia, and industry, and is committed to building equitable, public-interest digital infrastructures."
        },
        {
            "date": "Mon, Oct 20",
            "date2": "2025/10/20",
            "name": "Rebecca Portnoff",
            "affiliation": "Thorn",
            "link": "https://rebeccasportnoff.com/",
            "title": "Safety by Design for Generative AI: Preventing Child Sexual Abuse",
            "abstract": "Generative AI is being misused today to further sexual harm against children. 1 in 10 minors report knowing peers that have used generative AI to create explicit images of other kids; 11% of reports of sexual extortion (in which tactics were apparent) to the National Center of Missing and Exploited Children included threatening children with fake sexual imagery; 50% of Law Enforcement have encountered AI-generated child sexual abuse material used to groom minors; the Internet Watch Foundation observed a 10% increase in sadistic and hardcore AI-generated child sexual abuse material in the material they assessed from 2023 to 2024. Yet, we still have a window of opportunity to go down the right path with generative AI and ensure children are protected as the technology is built and globally adopted. This talk will unpack the tangible technical and policy solutions that can be implemented to prevent and combat AI-facilitated child sexual exploitation and abuse, detailing where and how these solutions are in place today and what gaps must still be addressed.",
            "bio": "Dr. Rebecca Portnoff is an expert on AI and child safety, responsible and ethical AI systems, and multi-stakeholder strategy for driving impact. She holds a B.S.E. from Princeton and a Ph.D from UC Berkeley, both in computer science, and is currently the Head of Data Science & AI at Thorn. Rebecca is an MIT Tech Review 35 under 35 innovator, and a Fast Company AI 20 technologist."
        },
        {
            "date": "Mon, Oct 27",
            "date2": "2025/10/27",
            "name": "Tingwei Zhang",
            "affiliation": "Cornell University",
            "link": "https://ztingwei.com/",
            "title": "Exposing and Exploiting Vulnerabilities in Multi-Modal Representations",
            "abstract": "Multi-modal embeddings align representations across different modalities (e.g., text, images, and audio) and serve as a core component of modern machine learning systems. However, their cross-modal nature also introduces new attack surfaces and security challenges. In this talk, I will present three works that expose vulnerabilities in multi-modal embeddings and propose potential defenses. 1) Adversarial Illusions: We show that embeddings can be attacked by perturbing an input to make its representation match a chosen target from another modality. These attacks are cross-modal, targeted, and compromise any downstream tasks. 2) Adversarial Hubs: We show that high-dimensional multi-modal spaces suffer from hubness—where certain points become spuriously similar to many others—and that attackers can exploit this property to inject adversarial “hub” content that dominates retrieval results or targets specific concepts. 3) Indirect Prompt Injection: We present indirect, cross-modal prompt injection attacks where hidden “meta-instructions” embedded in images influence the behavior of visual language models. These attacks enable adversaries to manipulate model interpretation and generation, leading to biased or harmful outputs.",
            "bio": "Tingwei Zhang is a third-year PhD student in Computer Science at Cornell Tech, advised by Professor Vitaly Shmatikov. His research focuses on security and privacy challenges in machine learning technologies, particularly in real-world scenarios and under adversarial conditions, with the goal of developing secure, ethical, and privacy-preserving AI systems."
        },
        {
            "date": "Mon, Nov 03",
            "date2": "2025/11/03",
            "name": "Kathrin Grosse",
            "affiliation": "IBM Research",
            "link": "https://research.ibm.com/people/kathrin-grosse",
            "title": "From Practical Machine Learning Security to AI Security Incident Reporting",
            "abstract": "Cybersecurity ensures the trustworthy and reliable functioning of digital systems. Currently, companies spend roughly 10% of their IT budget on cybersecurity. Thus, security becomes increasingly relevant also for emerging technologies like artificial intelligence (AI). Despite a large body of academic research, our current understanding of AI security has a critical gap. It does not cover how companies, public institutions, and non-profits use AI. This gap manifests as models are studied instead of pipelines, infeasible perturbations, or assumptions are unrealistic. This leaves us with a limited understanding of AI vulnerabilities. Meanwhile, attackers aren't waiting. They are already exploiting these vulnerabilities, and we discuss the evidence of these real-world AI security incidents. We thus discuss a proposal for an AI security incident reporting framework to create a practical understanding of AI security threats, allowing us to take a step towards trustworthy and secure AI.",
            "bio": "Kathrin Grosse is a Research Scientist at IBM Research, Zurich, Switzerland. Her research interests focus on AI security in the industry. Her work bridges research (in AI security) and industry needs. She received her master’s degree from Saarland University and her Ph.D. from Saarland University, in 2021 under the supervision of Michael Backes at CISPA Helmholtz Center. Following, she did a PostDoc with Battista Biggio in Cagliari, Italy; and Alexandre Alahi at EPFL, Switzerland. She interned with IBM in 2019 and Disney Research in 2020/21. As part of her work, she serves as a reviewer for, among others, IEEE S&P, Usenix Security, and ICML and organizes workshops at NeurIPS and ICML. In 2019, she was nominated as an AI Newcomer for the German Federal Ministry of Education and Research’s Science Year."
        },
        {
            "date": "Mon, Nov 10",
            "date2": "2025/11/10",
            "name": "Saeed Mahloujifar",
            "affiliation": "Meta FAIR",
            "link": "https://smahloujifar.github.io/",
            "title": "How Much Can Language Models Memorize?",
            "abstract": "What does it really mean when a language model “memorizes” its training data? Memorization remains one of the most puzzling behaviors of foundation models. In this talk, I’ll unpack the conceptual and practical challenges in defining memorization, propose a new definition based on Kolmogorov complexity, and show how this perspective helps us measure memorization efficiently and meaningfully. I’ll also share experiments that reveal surprising patterns in how large models internalize their data, with implications for privacy, generalization, and interpretability.",
            "bio": "Saeed Mahloujifar is a Research Scientist in the Fundamental AI Research (FAIR) team at Meta. His research interests are on theoretical foundations of privacy and security for AI systems and their interplay with cryptography. Previously, he was a postdoctoral researcher at Princeton University working with Prateek Mittal. He received his Ph.D. from the Department of Computer Science at the University of Virginia in the summer of 2020 under the supervision of Mohammad Mahmoody. Prior to UVa he got his B.Sc. degree from the department of Computer Engineering at Sharif University of Technology in the summer of 2015. He spent the summers of 2019 and 2020 working as a research intern at Microsoft Research, Redmond."
        },
        {
            "date": "Mon, Nov 17",
            "date2": "2025/11/17",
            "name": "Matthew Wright",
            "affiliation": "Rochester Institute of Technology",
            "link": "https://sites.google.com/site/matthewkwright/",
            "title": null,
            "abstract": null,
            "bio": null
        },
        {
            "date": "Mon, Nov 24",
            "date2": "2025/11/24",
            "name": "Ali Naseh",
            "affiliation": "UMass Amherst",
            "link": "https://ali7naseh.github.io/",
            "title": null,
            "abstract": null,
            "bio": null
        },
        {
            "date": "Mon, Dec 01",
            "date2": "2025/12/01",
            "name": "Cassidy Gibson",
            "affiliation": "University of Florida",
            "link": "https://www.linkedin.com/in/cassidy-gibson-409a422b6/",
            "title": null,
            "abstract": null,
            "bio": null
        },
        {
            "date": "Mon, Dec 08",
            "date2": "2025/12/08",
            "name": "UMass Research Talk",
            "affiliation": "UMass Amherst",
            "link": null,
            "title": null,
            "abstract": null,
            "bio": null
        }
    ],
    "S25": [
        {
            "date": "Mon, Feb 03",
            "date2": "2025/02/03",
            "name": "Anshuman Suri",
            "affiliation": "Northeastern University",
            "link": "https://www.anshumansuri.com/",
            "title": "White-box vs Black-box: Privacy Auditing for Machine Learning",
            "abstract": "Machine learning models pose privacy risks through memorization, with  membership inference being the most studied threat—determining whether a specific record was in the training data. State-of-the-art attacks assume black-box access, and prior theoretical work suggests that parameter access is unnecessary for optimal membership inference. This view is reinforced by prevailing research folklore, with little work exploring MIAs under parameter access. In this talk, I will challenge these assumptions and demonstrate that, contrary to common belief, optimal membership inference does require parameter access. I will then discuss the implications for privacy auditing and how it differs from inference attacks designed for adversarial or demonstrative purposes.",
            "bio": "Anshuman Suri is a postdoctoral fellow at Northeastern University's Khoury College of Computer Sciences, working with Alina Oprea. He earned his PhD from the University of Virginia in 2024 under David Evans, focusing on security and privacy in machine learning. His research spans membership inference, user inference, and attacks on large language models. He has worked as an Applied Scientist at Microsoft and interned at Oracle Research. His work has received several honors, including the John A. Stankovic Graduate Research Award. He has also served as a reviewer for top ML conferences, earning Outstanding Reviewer awards at ICLR, ICML, and ICCV."
        },
        {
            "date": "Mon, Feb 10",
            "date2": "2025/02/10",
            "name": "Sahar Abdelnabi",
            "affiliation": "Microsoft",
            "link": "https://s-abdelnabi.github.io/",
            "title": "Evaluating and Securing LLM-Agentic Networks",
            "abstract": "There is an increasing interest in using LLM agents to autonomously automate tasks and workflows. For example, the new OpenAI operator may be used to design travel plans for the user. Service providers now use LLM chatbots to assist users as well. Soon, it is very likely that these two sides are going to communicate, forming agentic networks. Such paradigms will unlock new use cases where agents can negotiate, deliberate, adapt, and find creative solutions on behalf of entities they represent. In this talk, I will discuss our work on evaluating multi-agent negotiations, and how that can be beneficial to test reasoning and create evolving, dynamic benchmarks. We use this benchmark to study manipulation and safety risks, such as how cooperative agents can be steered by greedy or adversarial ones. In the second part of the talk, I will present our new work to identify security and privacy risks in adaptive agentic networks where an assistant communicates with an external party to fulfil a multi-goal task. The assistant must perform actions that are entailed by the goal, not over share information, and maintain utility against greedy agents. We create a firewalling mitigation that allow agents to dynamically communicate and adapt, while balancing these security and privacy risks.",
            "bio": "Sahar Abdelnabi is an AI security researcher at Microsoft's Security Response Center. She completed her PhD at the CISPA Helmholtz Center for Information  Security under the supervision of Prof. Dr. Mario Fritz and holds an MSc from Saarland University. Her research focuses on the intersection of machine learning with security, safety, and sociopolitical aspects, including understanding and mitigating failure modes of machine learning models, addressing biases, and exploring emergent safety challenges posed by large language models."
        },
        {
            "date": "Thu, Feb 20",
            "date2": "2025/02/20",
            "name": "Javier Rando",
            "affiliation": "ETH Zurich",
            "link": "https://javirando.com/",
            "title": "Gradient-based Jailbreak Images for Multimodal Fusion Models",
            "abstract": "Augmenting language models with image inputs may enable more effective jailbreak attacks through continuous optimization, unlike text inputs that require discrete optimization. However, new multimodal fusion models tokenize all input modalities using non-differentiable functions, which hinders straightforward attacks. In this work, we introduce the notion of a tokenizer shortcut that approximates tokenization with a continuous function and enables continuous optimization. We use tokenizer shortcuts to create the first end-to-end gradient image attacks against multimodal fusion models. We evaluate our attacks on Chameleon models and obtain jailbreak images that elicit harmful information for 72.5% of prompts. Jailbreak images outperform text jailbreaks optimized with the same objective and require 3x lower compute budget to optimize 50x more input tokens. Finally, we find that representation engineering defenses, like Circuit Breakers, trained only on text attacks can effectively transfer to adversarial image inputs.",
            "bio": "Javier Rando is a doctoral student at ETH Zurich, advised by Florian Tramèr and Mrinmaya Sachan. His research focuses on identifying potential failures in deploying advanced AI models in real-world applications, particularly through red-teaming large language models. His PhD is supported by the ETH AI Center Doctoral Fellowship. In the summer of 2024, he interned with Meta's GenAI Safety & Trust team. Javier holds an MSc in Computer Science from ETH Zurich and a BSc in Data Science from Pompeu Fabra University. He has also been a visiting researcher at NYU under He He and founded EXPAI, an explainable AI startup in Spain. He has received the spotlight award for one of his recent papers at ICLR 2025."
        },
        {
            "date": "Mon, Feb 24",
            "date2": "2025/02/24",
            "name": "Norman Mu",
            "affiliation": "xAI",
            "link": "https://www.normanmu.com/",
            "title": "A Closer Look at System Prompt Robustness",
            "abstract": "System prompts have emerged as a critical control surface for specifying the behavior of LLMs in chat and agent settings. Developers depend on system prompts to specify important context, output format, personalities, guardrails, content policies, and safety countermeasures, all of which require models to robustly adhere to the system prompt, especially when facing conflicting or adversarial user inputs. In practice, models often forget to consider relevant guardrails or fail to resolve conflicting demands between the system and the user. In this work, we study various methods for improving system prompt robustness by creating realistic new evaluation and fine-tuning datasets based on prompts collected from OpenAI's GPT Store and HuggingFace's HuggingChat. Our experiments assessing models with a panel of new and existing benchmarks show that performance can be considerably improved with realistic fine-tuning data, as well as inference-time interventions such as classifier-free guidance. Finally, we analyze the results of recently released reasoning models from OpenAI and DeepSeek, which show exciting but uneven improvements on the benchmarks we study. Overall, current techniques fall short of ensuring system prompt robustness and further study is warranted.",
            "bio": "Norman Mu is a Member of Technical Staff at xAI, focusing on AI safety. He earned his Ph.D. in Computer Science from the University of California, Berkeley, where he was supported by the NSF Graduate Research Fellowship. During his doctoral studies, he was also a visiting researcher at Facebook AI Research. He holds a bachelor's degree in Computer Science from the University of California, Berkeley. His research interests include robustness and uncertainty in machine learning models."
        },
        {
            "date": "Mon, Mar 03",
            "date2": "2025/03/03",
            "name": "Harsh Chaudhari",
            "affiliation": "Northeastern University",
            "link": "https://harshch1803.github.io/",
            "title": "Propagation of Adversarial Bias to Distilled Language Models",
            "abstract": "The widespread deployment of Large Language Models (LLMs) trained by knowledge distillation is increasingly raising concerns about their resilience to adversarial manipulation.This paper investigates the vulnerability of distilled language models to adversarial injection of biased content during training. More broadly, we demonstrate how malicious vendors can inject  adversarial biased data into a large ``teacher'' LLM's training set, causing the adversarial bias to not only propagate to a smaller student model, but also become amplified. Using  data poisoning techniques, we manipulate the teacher's output to include adversarial bias in the generated content, such as promoting a particular brand or generating phishing links. We show that the attack transfers to the student model, where the adversarial bias becomes even more pronounced and impacts unseen tasks. With only 25 poisoned samples, or 0.25% poisoning rate in the teacher's training data, the student model generates a large fraction 76.9% of biased responses. Moreover, the student model's fraction of biased responses is 8.1x higher on unseen tasks compared to the teacher model. Our findings highlight significant security and trustworthiness concerns for distilled language models deployed in user-facing applications.",
            "bio": "Harsh Chaudhari is a fourth-year PhD student in Computer Science at Northeastern University, under the supervision of Professor Alina Oprea. His research focuses on the security and privacy of machine learning models, particularly in understanding and mitigating threats through adversarial attacks. Harsh has had several internship experiences, the most recent of which was at Google Deepmind, where he worked on adversarial bias in language models. He has published his works at notable venues such as ICLR, Oakland, and NDSS."
        },
        {
            "date": "Mon, Mar 10",
            "date2": "2025/03/10",
            "name": "Andy Zou",
            "affiliation": "Carnegie Mellon Univerosity",
            "link": "https://andyzoujm.github.io/",
            "title": "Red Teaming AI Agents in-the-wild: Revealing Deployment Vulnerabilities",
            "abstract": "This presentation demonstrates how red teaming uncovers critical vulnerabilities in AI agents that challenge assumptions about safe deployment. The talk discusses the risks of integrating AI into real-world applications and recommends practical safeguards to enhance resilience and ensure dependable deployment in high-risk settings.",
            "bio": "Andy Zou is a PhD student at CMU. He is the CTO and cofounder at Gray Swan AI and a cofounder of Center for AI Safety. He works in AI Safety and Security."
        },
        {
            "date": "Mon, Mar 24",
            "date2": "2025/03/24",
            "name": "Xiangyu Qi",
            "affiliation": "OpenAI",
            "link": "https://xiangyuqi.com/",
            "title": "Safety Alignment Should Be Made More Than Just A Few Tokens Deep",
            "abstract": "The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks can jailbreak aligned models. In this talk, I will show that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens --- which we refer to as 'shallow safety alignment'. I will present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. I will also show how these findings help explain multiple recently discovered vulnerabilities in LLMs. After that, I will discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits.",
            "bio": "Xiangyu Qi is a Member of Technical Staff at OpenAI, focusing on AI safety, security, and alignment. He completed his Ph.D. in Electrical and Computer Engineering at Princeton University in February 2025, under the guidance of Professors Prateek Mittal and Peter Henderson. His doctoral research centered on identifying vulnerabilities in AI systems and developing mitigation strategies, with his work being published in prestigious conferences such as CVPR, AAAI, and ICLR. Dr. Qi's research has also been highlighted in mainstream media outlets, including The New York Times. In addition to this, he gained industry experience through research internships at Google DeepMind and Amazon, where he contributed to enhancing the robustness of AI systems. He holds a bachelor's degree from Zhejiang University, earned in 2021."
        },
        {
            "date": "Mon, Mar 31",
            "date2": "2025/03/31",
            "name": "Om Thakkar",
            "affiliation": "OpenAI",
            "link": "http://www.omthakkar.com/",
            "title": "Privacy Leakage in Speech Models: Attacks and Mitigations",
            "abstract": "Recent research has highlighted the vulnerability of neural networks to unintended memorization of training examples, raising significant privacy concerns. In this talk, we first explore two primary types of privacy leakage: extraction attacks and memorization audits. Specifically, we examine novel extraction attacks targeting speech models and discuss efficient methodologies for auditing memorization. In the second half of the talk, we will present empirical privacy approaches that enable training state-of-the-art speech models while effectively reducing memorization risks.",
            "bio": "Om Thakkar is a Member of Technical Staff at OpenAI, specializing in privacy-preserving AI research with a focus on differential privacy and its applications to deep learning in production systems. Prior to joining OpenAI, he was a Senior Research Scientist at Google. He earned his Ph.D. in Computer Science from Boston University in 2019, under the guidance of Dr. Adam Smith. His doctoral research focused on differential privacy and its applications to machine learning. Dr. Thakkar holds a B.Tech. in Information and Communication Technology from the Dhirubhai Ambani Institute in India, completed in 2014. He has published his research in leading conferences such as the IEEE Symposium on Security and Privacy (S&P), NeurIPS, and ICML and has several patents under his name."
        },
        {
            "date": "Mon, Apr 07",
            "date2": "2025/04/07",
            "name": "Ryan McKenna",
            "affiliation": "Google",
            "link": "http://linkedin.com/in/ryan-mckenna-76772a57",
            "title": "Private Analytics and Learning at Google",
            "abstract": "In this talk, I will give a broad overview of how we think about the many dimensions of data privacy at Google, discuss some of the private analytics and learning problems we've faced here, and go over some of the research we have done in this space to solve these problems, as well as promising open research directions. To that end, I will talk about how we used our federated analytics platform to derive environmental insights from on-device location data and how Gboard trains their next-word prediction model with strong differential privacy guarantees and without centralized data collection. I will then overview a body of research we have done on improving DP-SGD, the most widely used mechanism for training machine learning models with differential privacy.",
            "bio": null
        },
        {
            "date": "Mon, Apr 14",
            "date2": "2025/04/14",
            "name": "Milad Nasr",
            "affiliation": "Google",
            "link": "https://www.linkedin.com/in/milad-nasr-50ab6a56",
            "title": "Rethinking Exploitation in the Age of Large Language Models",
            "abstract": "Large Language Models (LLMs) present a significant potential to transform the exploit monetization landscape, enabling more sophisticated attack vectors beyond current methodologies. Whereas traditional attacks often target the lowest common denominator (e.g., broad ransomware deployment), LLM-driven approaches could facilitate highly targeted attacks, such as analyzing compromised personal data to identify and leverage sensitive information for blackmail, potentially followed by encryption. Furthermore, LLMs could automate the discovery of vulnerabilities across a multitude of niche products, contrasting with the manual effort required to find complex bugs in high-profile systems. We provide proof-of-concept implementations demonstrating these capabilities, including an instance where an LLM autonomously identified sensitive personal correspondence within the Enron email dataset without human guidance. While the current cost structure renders these LLM-based attacks economically challenging for widespread scaling, the decreasing cost of LLM technology presents a growing economic incentive for their use.",
            "bio": "Milad Nasr is a Research Scientist at Google DeepMind, specializing in machine learning security and privacy. He earned his Ph.D. in Computer Science from the University of Massachusetts Amherst in 2021, where he was advised by Professor Amir Houmansadr. Milad’s research focuses on developing attacks and defenses for privacy vulnerabilities in AI systems, with a particular emphasis on membership inference, data leakage, and adversarial robustness. His work has been published in leading venues such as IEEE S&P, USENIX Security, ICML, and NeurIPS. He won the Outstanding Paper Award at NeurIPS 2023 and the Distinguished Paper Award at Usenix Security 2023. He has also contributed to high-profile projects, including the Gemini multimodal models."
        },
        {
            "date": "Fri, Apr 18",
            "date2": "2025/04/18",
            "name": "Edoardo Debenedetti",
            "affiliation": "ETH Zurich",
            "link": "https://edoardo.science/",
            "title": "Defeating Prompt Injections by Design",
            "abstract": "Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models may be susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL relies on a notion of a capability to prevent the exfiltration of private data over unauthorized data flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks with provable security in AgentDojo [NeurIPS 2024], a recent agentic security benchmark.",
            "bio": "Edoardo Debenedetti is a PhD student in Computer Science at ETH Zurich, advised by Prof. Florian Tramèr, where his research focuses on real-world machine learning security and privacy, mostly from an offensive point of view. Most recently, he's been looking into the security of AI agents. His research is funded by a CYD Doctoral Fellowship from the Armasuisse Cyber-Defense Campus. He recently worked as a Student Researcher at Google, focusing on AI agent security with the AI Red Team and DeepMind. His work has been published at major security and machine learning conferences, receiving recognitions such as a Distinguished Paper Runner-up at SaTML 2024 and a Spotlight at NeurIPS 2024."
        },
        {
            "date": "Mon, Apr 28",
            "date2": "2025/04/28",
            "name": "Jonas Geiping",
            "affiliation": "ELLIS Institute",
            "link": "https://jonasgeiping.github.io/",
            "title": "Increasing Trust through New Benchmarks and Measuring Model Oversight",
            "abstract": "In this talk, I want to talk about recent work in posing relevant benchmarks to modern model behavior that complement existing evaluations. I want to then talk about generalizations into broader domains, which require model oversight. We'll discuss how model oversight is limited by model similarity, and how to correctly quantify model similarity.",
            "bio": "Jonas Geiping is a Machine Learning researcher in Tübingen, where he leads the research group for safety- & efficiency-aligned learning. Before this, he has spent time at the Universities of Maryland, Siegen, and Münster."
        },
        {
            "date": "Mon, May 05",
            "date2": "2025/05/05",
            "name": "PhD Students",
            "affiliation": "UMass Amherst",
            "link": "#",
            "title": "Lightning research talks on a variety of topics (e.g. MIA, DP, privacy, watermarking, jailbreaking)",
            "abstract": "In our last seminar, we will have our very own UMass PhD students present their research in AI security and privacy. Topics include: Optimizing Private Measurements for DP Synthetic Data, Synthesizing Cryptographic and NLP LLM Watermarking, Semantic Hashing: Private Image-to-Image Retrieval, Can LLMs Really Recognize Your Names?, Stealthy Membership Inference for Retrieval-Augmented Generation, Multilingual and Multi-Accent Jailbreaking of Audio LLMs. We'll also have some snacks and coffee!",
            "bio": null
        }
    ]
}
